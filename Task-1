 1: Set Up Your EnvironmentEnsure you have Python installed and install the necessary libraries:
pip install pandas numpy scikit-learn matplotlib seaborn

2: Load and Preprocess the DataWe'll use the German Credit Data dataset, which you can download from the UCI Machine Learning Repository.import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'
columns = ['Status of existing checking account', 'Duration in month', 'Credit history', 'Purpose',
           'Credit amount', 'Savings account/bonds', 'Present employment since', 'Installment rate in percentage of disposable income',
           'Personal status and sex', 'Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',
           'Other installment plans', 'Housing', 'Number of existing credits at this bank', 'Job', 'Number of people being liable to provide maintenance for',
           'Telephone', 'Foreign worker', 'Creditability']
data = pd.read_csv(url, delim_whitespace=True, header=None, names=columns)

# Encode categorical variables
label_encoders = {}
for column in data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    data[column] = label_encoders[column].fit_transform(data[column])

# Separate features and target variable
X = data.drop('Creditability', axis=1)
y = data['Creditability']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

3: Choose and Train the ModelWe'll use a Random Forest classifier for this example. You can try other classifiers like Logistic Regression, Gradient Boosting, or Neural Networks.
from sklearn.ensemble import RandomForestClassifier

# Initialize and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

4: Evaluate the ModelAssess the model's accuracy and other performance metrics.
# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot feature importance
feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=data.columns[:-1],
                                   columns=['importance']).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances.importance, y=feature_importances.index)
plt.title('Feature Importances')
plt.show()

5: Save the ModelSave the trained model for future use.
import joblib

# Save the model to a file
joblib.dump(model, 'credit_scoring_model.pkl')

# Save the scaler for future use
joblib.dump(scaler, 'scaler.pkl')

6: Load and Use the ModelLoad the model and scaler to make predictions on new data.
# Load the model and scaler
model = joblib.load('credit_scoring_model.pkl')
scaler = joblib.load('scaler.pkl')

# Example new data (replace with actual new data)
new_data = pd.DataFrame({
    'Status of existing checking account': [1],
    'Duration in month': [18],
    'Credit history': [2],
    'Purpose': [0],
    'Credit amount': [1049],
    'Savings account/bonds': [2],
    'Present employment since': [3],
    'Installment rate in percentage of disposable income': [4],
    'Personal status and sex': [2],
    'Other debtors / guarantors': [0],
    'Present residence since': [4],
    'Property': [2],
    'Age in years': [67],
    'Other installment plans': [0],
    'Housing': [1],
    'Number of existing credits at this bank': [1],
    'Job': [3],
    'Number of people being liable to provide maintenance for': [1],
    'Telephone': [1],
    'Foreign worker': [1]
})

# Encode categorical variables
for column in new_data.select_dtypes(include=['object']).columns:
    new_data[column] = label_encoders[column].transform(new_data[column])

# Standardize the new data
new_data_scaled = scaler.transform(new_data)
# Make a prediction
prediction = model.predict(new_data_scaled)
print("Prediction (1=creditworthy, 0=not creditworthy):", prediction[0])
